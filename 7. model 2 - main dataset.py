# -*- coding: utf-8 -*-
"""CEAT - model2 (base dataset).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P1XfHDEyudR-yMm75JB20b0CKdb5WHtZ
"""

# Install required libraries
!pip install catboost xgboost shap optuna

# Imports
import pandas as pd
import numpy as np
import shap
import optuna
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from catboost import CatBoostClassifier
from xgboost import XGBClassifier

import warnings
warnings.filterwarnings("ignore")

# Upload dataset
uploaded = files.upload()
file_name = list(uploaded.keys())[0]

# Read dataset
if file_name.endswith('.xlsx') or file_name.endswith('.xls'):
    df = pd.read_excel(file_name, engine='openpyxl')
elif file_name.endswith('.csv'):
    df = pd.read_csv(file_name)
else:
    raise ValueError("Upload a .xlsx, .xls or .csv file.")

# Check for missing values
print("Missing values:\n", df.isnull().sum())

# Set target column name (change if needed)
target_col = 'Psychological Stress and Frustration'  # Update to your column name

# Scale features
scaler = RobustScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(df.drop(columns=[target_col])), columns=df.columns[:-1])
X_scaled[target_col] = df[target_col]

# Split features and target
X = X_scaled.drop(columns=[target_col])
y = X_scaled[target_col]

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

model = CatBoostClassifier(iterations=200, depth=6, learning_rate=0.1, verbose=False)
model.fit(X_train, y_train)

# SHAP
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_train)

# For multiclass SHAP
if isinstance(shap_values, list):
    shap.summary_plot(shap_values[1], X_train)  # Pick class 1, or loop for all
else:
    shap.summary_plot(shap_values, X_train)

# Define Base Models
catboost = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.05, verbose=False) # Removed class_weights='Balanced'
xgboost = XGBClassifier(n_estimators=500, max_depth=6, learning_rate=0.05,
                        use_label_encoder=False, eval_metric='mlogloss')
extratrees = ExtraTreesClassifier(n_estimators=500, max_depth=6, class_weight='balanced')

# Meta Learner
meta_learner = LogisticRegression(class_weight='balanced', max_iter=1000)

# Stacking Classifier
stack_model = StackingClassifier(
    estimators=[("catboost", catboost), ("xgboost", xgboost), ("extratrees", extratrees)],
    final_estimator=meta_learner,
    cv=5,
    passthrough=True
)

# Train
stack_model.fit(X_train, y_train)

y_pred = stack_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

def objective(trial):
    params = {
        'iterations': trial.suggest_int('iterations', 100, 700),
        'depth': trial.suggest_int('depth', 4, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10)
    }

    # Split again inside for CV
    X_tune, X_val, y_tune, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=trial.number)

    model = CatBoostClassifier(**params, verbose=False)
    model.fit(X_tune, y_tune)
    return model.score(X_val, y_val)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30)

print("Best parameters found:", study.best_params)
