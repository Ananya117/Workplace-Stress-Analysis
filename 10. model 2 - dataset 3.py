# -*- coding: utf-8 -*-
"""CEAT - model2 (dataset 3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GaRlY25eyMlGlG6Tj3kU8EQyp-cwSKWu
"""

!pip install -q catboost xgboost shap optuna

import pandas as pd, numpy as np, shap, optuna, warnings
import matplotlib.pyplot as plt
from google.colab import files

from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from catboost import CatBoostClassifier
from xgboost import XGBClassifier
warnings.filterwarnings("ignore")

import pandas as pd
from google.colab import files

uploaded = files.upload()
file_name = list(uploaded.keys())[0]

# Correct way to read Excel:
df = pd.read_excel(file_name)

# ═════════════════  PRE‑PROCESSING FOR FRIEND’S DATASET  ═════════════════
TARGET_COL = "Psychological Stress and Frustration"

# ── 1.  Drop rows with no target ─────────────────────────────────────────
df = df.dropna(subset=[TARGET_COL])

# ── 2.  (Optional) binarise target  >3 → 1  else 0  ─────────────────────
# df[TARGET_COL] = (df[TARGET_COL] > 3).astype(int)

# ── 3.  Identify categorical feature columns ────────────────────────────
CATEGORICAL_COLS = ["Country", "Exercise Level",
                    "Diet Type", "Mental Health Condition"]

# ── 4.  Ordinal‑encode those categoricals (CatBoost can take raw strings
#        but XGB / ExtraTrees need numbers; ordinal is fine here) ─────────
from sklearn.preprocessing import OrdinalEncoder

ord_enc = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
df[CATEGORICAL_COLS] = ord_enc.fit_transform(df[CATEGORICAL_COLS].astype(str))

# ── 5.  Split features / target ─────────────────────────────────────────
X = df.drop(columns=[TARGET_COL])
y = df[TARGET_COL]

# ── 6.  Impute remaining NaNs then scale numerics (robust to outliers) ──
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler

num_imputer = SimpleImputer(strategy="median")
X = pd.DataFrame(num_imputer.fit_transform(X), columns=X.columns)

scaler = RobustScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# ------------- continue with train_test_split, stack_model.fit, etc. -----

# ─── 5. Train/test split ──────────────────────────────────────
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42)

print(f"Train rows: {len(X_train)}   •  Test rows: {len(X_test)}")

# ─── 6. Build stack ───────────────────────────────────────────
catboost   = CatBoostClassifier(iterations=500, depth=6,
                                learning_rate=0.05, verbose=False)
xgboost    = XGBClassifier(n_estimators=500, max_depth=6,
                           learning_rate=0.05, use_label_encoder=False,
                           eval_metric='mlogloss')
extratrees = ExtraTreesClassifier(n_estimators=500, max_depth=6,
                                  class_weight='balanced')
meta = LogisticRegression(max_iter=1000, class_weight='balanced')

stack = StackingClassifier(
    estimators=[("cat", catboost),
               ("xgb", xgboost),
               ("et",  extratrees)],
    final_estimator=meta,
    cv=5,
    passthrough=True)

# ─── 7. Fit & evaluate ────────────────────────────────────────
stack.fit(X_train, y_train)
y_pred = stack.predict(X_test)

print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification report:\n",
      classification_report(y_test, y_pred,
                            target_names=le_target.classes_))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# ─── 8. SHAP on CatBoost component (optional) ────────────────
explainer   = shap.TreeExplainer(stack.named_estimators_["cat"])
shap_values = explainer.shap_values(X_train)
shap.summary_plot(shap_values, X_train)
