# -*- coding: utf-8 -*-
"""CEAT - model2 (dataset 1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IenEjnd9bPYgJJnMKGQNyDpHaAEfEv8M
"""

!pip install -q catboost xgboost shap optuna

import pandas as pd, numpy as np, shap, optuna, warnings
import matplotlib.pyplot as plt
from google.colab import files                         # <── for file upload
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
warnings.filterwarnings("ignore")

# ─── 1. Upload & read dataset ─────────────────────────────────
uploaded  = files.upload()
file_name = list(uploaded.keys())[0]
df = pd.read_csv(file_name)
print("Missing values:\n", df.isnull().sum())

# ═════════════════ PRE‑PROCESSING: Sleep‑Health dataset ════════════════
TARGET_COL       = "Stress Level"      # numeric 0‑10
ID_COLS          = ["Person ID"]       # drop identifiers
CATEGORICAL_COLS = ["Gender", "Occupation", "BMI Category",
                    "Blood Pressure", "Sleep Disorder"]

# 1) Drop identifier columns
df = df.drop(columns=ID_COLS)

# 2) Drop rows without the target
df = df.dropna(subset=[TARGET_COL])

# 3) (Optional) turn Stress Level into binary high/low
# df[TARGET_COL] = (df[TARGET_COL] > 5).astype(int)   # uncomment to binarise

# 4) Ordinal‑encode categorical features
from sklearn.preprocessing import OrdinalEncoder
ord_enc = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
df[CATEGORICAL_COLS] = ord_enc.fit_transform(df[CATEGORICAL_COLS].astype(str))

# 5) Impute any remaining NaNs with median, then scale numerics
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler

num_imputer = SimpleImputer(strategy="median")
df[df.columns] = num_imputer.fit_transform(df)   # impute all cols (safe)

num_cols = df.columns.drop(CATEGORICAL_COLS + [TARGET_COL])  # pure numeric cols
scaler   = RobustScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

# 6) Split features / target for the model stack
X = df.drop(columns=[TARGET_COL])
y = df[TARGET_COL]

# -------------- continue with train_test_split, stack.fit, SHAP, etc. -----

# ─── 3. Train‑test split ───────────────────────────────────────
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42)

# ─── 4. Define base learners ──────────────────────────────────
catboost   = CatBoostClassifier(iterations=500, depth=6,
                                learning_rate=0.05, verbose=False)
xgboost    = XGBClassifier(n_estimators=500, max_depth=6,
                           learning_rate=0.05, use_label_encoder=False,
                           eval_metric='mlogloss')
extratrees = ExtraTreesClassifier(n_estimators=500, max_depth=6,
                                  class_weight='balanced')
meta_learner = LogisticRegression(max_iter=1000, class_weight='balanced')

stack_model = StackingClassifier(
    estimators=[("catboost", catboost),
               ("xgboost",  xgboost),
               ("extratrees", extratrees)],
    final_estimator=meta_learner,
    cv=5, passthrough=True)

print("Accuracy:", accuracy_score(y_test, y_pred))

# Build readable class names automatically
unique_classes = sorted(np.unique(np.concatenate([y_test, y_pred])))
if len(unique_classes) == 2:
    target_names = ['Low Stress', 'High Stress']
elif len(unique_classes) == 3:
    target_names = ['Low', 'Moderate', 'High']
else:
    target_names = [f'Level {c}' for c in unique_classes]

print("\nClassification Report:\n",
      classification_report(y_test, y_pred, target_names=target_names))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ─── 6. SHAP summary for CatBoost base learner ────────────────
explainer   = shap.TreeExplainer(stack_model.named_estimators_["catboost"])
shap_values = explainer.shap_values(X_train)
shap.summary_plot(shap_values, X_train)
