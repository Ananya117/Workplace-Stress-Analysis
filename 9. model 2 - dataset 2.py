# -*- coding: utf-8 -*-
"""CEAT - model2 (dataset 2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jwt701FOVe9Kgpi9f2QWkdjD36kx-E6Z
"""

!pip install -q catboost xgboost shap optuna

import pandas as pd, numpy as np, shap, optuna, warnings
import matplotlib.pyplot as plt
from google.colab import files                         # <── for file upload
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
warnings.filterwarnings("ignore")

# ─── 1. Upload & read dataset ─────────────────────────────────
uploaded  = files.upload()
file_name = list(uploaded.keys())[0]
df = pd.read_csv(file_name)
print("Missing values:\n", df.isnull().sum())

# ═══════════ PRE‑PROCESSING: Employee‑Metrics (Binary Target) ═══════════
TARGET_COL   = "Stress_Level"        # original numeric 1‑10
ID_COLS      = ["Employee_ID"]       # identifier to drop

# 1) Drop identifier column(s)
df = df.drop(columns=ID_COLS)

# 2) Remove rows without the target
df = df.dropna(subset=[TARGET_COL])

# 3) Binary target: High Stress (1) if >5, else Low Stress (0)
df[TARGET_COL] = (df[TARGET_COL] > 5).astype(int)

# 4) Median‑impute any missing values, then robust‑scale numerics
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler

imputer = SimpleImputer(strategy="median")
df[df.columns] = imputer.fit_transform(df)

feature_cols = df.columns.drop(TARGET_COL)
scaler       = RobustScaler()
df[feature_cols] = scaler.fit_transform(df[feature_cols])

# 5) Split features / target for the model stack
X = df.drop(columns=[TARGET_COL])
y = df[TARGET_COL]
# ─────────────────────────────────────────────────────────────────────────
# Continue with train_test_split, model definitions, stack.fit, etc.

# ─── 3. Train‑test split ───────────────────────────────────────
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42)

# ─── 4. Define base learners ──────────────────────────────────
catboost   = CatBoostClassifier(iterations=500, depth=6,
                                learning_rate=0.05, verbose=False)
xgboost    = XGBClassifier(n_estimators=500, max_depth=6,
                           learning_rate=0.05, use_label_encoder=False,
                           eval_metric='mlogloss')
extratrees = ExtraTreesClassifier(n_estimators=500, max_depth=6,
                                  class_weight='balanced')
meta_learner = LogisticRegression(max_iter=1000, class_weight='balanced')

stack_model = StackingClassifier(
    estimators=[("catboost", catboost),
               ("xgboost",  xgboost),
               ("extratrees", extratrees)],
    final_estimator=meta_learner,
    cv=5, passthrough=True)

# ─── 5. Fit & evaluate ────────────────────────────────────────
stack_model.fit(X_train, y_train)
y_pred = stack_model.predict(X_test)  # <── this defines y_pred


print("Accuracy:", accuracy_score(y_test, y_pred))

# Build readable class names automatically
unique_classes = sorted(np.unique(np.concatenate([y_test, y_pred])))
if len(unique_classes) == 2:
    target_names = ['Low Stress', 'High Stress']
elif len(unique_classes) == 3:
    target_names = ['Low', 'Moderate', 'High']
else:
    target_names = [f'Level {c}' for c in unique_classes]

print("\nClassification Report:\n",
      classification_report(y_test, y_pred, target_names=target_names))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ─── 6. SHAP summary for CatBoost base learner ────────────────
explainer   = shap.TreeExplainer(stack_model.named_estimators_["catboost"])
shap_values = explainer.shap_values(X_train)
shap.summary_plot(shap_values, X_train)
