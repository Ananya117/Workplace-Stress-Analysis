# -*- coding: utf-8 -*-
"""CEAT - model4 for remote work alone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IHmEBkYLHfD7dHTCmPTVyxauXa1KJg2e
"""

# %% ───────────────────────────── INSTALL DEPENDENCIES ───────────────────────────
!pip install -q catboost optuna pytorch-tabnet shap seaborn scikit-learn --upgrade

# %% ─────────────────────────────── IMPORTS & CONFIG ─────────────────────────────
import io, warnings, os, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from google.colab import files
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    accuracy_score, f1_score, log_loss,
    classification_report, confusion_matrix)
from sklearn.preprocessing import LabelEncoder
from catboost import CatBoostClassifier, Pool
from pytorch_tabnet.tab_model import TabNetClassifier
import optuna, shap, torch

warnings.filterwarnings('ignore')
plt.rcParams['figure.dpi'] = 110
TARGET_COL   = 'Stress_Level'     # <- change if needed
CLASS_LABELS = list(range(1,11))   # 1‑10 classes
TEST_SIZE    = 0.15
RANDOM_SEED  = 42
SENSITIVE_COLS = ['Gender', 'AgeGroup']  # fairness slices (optional)

# %% ──────────────────────────── UPLOAD & READ THE DATA ──────────────────────────
print('▶️  Choose your CSV …')
uploaded = files.upload(); assert uploaded, 'No file uploaded!'
df = pd.read_csv(io.BytesIO(next(iter(uploaded.values()))))
print('Shape:', df.shape)

# %% ────────────────────────────── BASIC CLEAN‑UP / EDA ──────────────────────────
cat_cols = df.select_dtypes(include=['object']).columns.tolist()
num_cols = [c for c in df.columns if c not in cat_cols+[TARGET_COL]]
for col in cat_cols: df[col] = df[col].astype('category').fillna(df[col].mode()[0])
for col in num_cols: df[col].fillna(df[col].median(), inplace=True)

print('\n▶️  Correlation vs. target')
_corr = df.copy()
for c in cat_cols: _corr[c] = _corr[c].cat.codes
print(_corr.corr(numeric_only=True)[TARGET_COL].sort_values(ascending=False).head(15))
plt.figure(figsize=(8,6)); sns.heatmap(_corr.corr(), cmap='coolwarm', center=0); plt.title('Correlation Heat‑map'); plt.show()

# %% ─────────────────────── PREP FEATURES & LABELS ───────────────────────────────
X = df.drop(columns=[TARGET_COL])
# map 1‑10 ➜ 0‑9
label_map = {v:i for i,v in enumerate(CLASS_LABELS)}
y = df[TARGET_COL].map(label_map).values
cat_idx = [X.columns.get_loc(c) for c in cat_cols]
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED)

# %% ───────────────────────────── CATBOOST CLASSIFIER ────────────────────────────
print('▶️ Tuning CatBoost …')

def catboost_objective(trial):
    params = {
        # Optuna‑tuned hyper‑params (all valid CatBoost names)
        'iterations':        3000,
        'depth':             trial.suggest_int('depth', 4, 10),
        'learning_rate':     trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
        'l2_leaf_reg':       trial.suggest_float('l2_leaf_reg', 1, 10),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),
        'loss_function':     'MultiClass',
        'eval_metric':       'Accuracy',
        'cat_features':      cat_idx,
        'random_seed':       RANDOM_SEED,
        'verbose':           0
    }
    model = CatBoostClassifier(**params)
    model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=200)
    return model.best_score_['validation']['Accuracy']

study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))
study.optimize(catboost_objective, n_trials=25, show_progress_bar=False)

best_cb_params = study.best_params
best_cb_params.update({'iterations':3000, 'loss_function':'MultiClass', 'eval_metric':'Accuracy',
                       'cat_features':cat_idx, 'random_seed':RANDOM_SEED, 'verbose':200})

cb = CatBoostClassifier(**best_cb_params)
cb.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=200)

# ─── Metrics ────────────────────────────────────────────────────────────────────
cb_pred_prob = cb.predict_proba(X_val)
cb_pred      = cb_pred_prob.argmax(axis=1)
print('CatBoost  —  Accuracy:', accuracy_score(y_val, cb_pred), '  Macro‑F1:', f1_score(y_val, cb_pred, average='macro'))
print(classification_report(y_val, cb_pred, digits=3))

plt.plot(cb.get_evals_result()['learn']['Accuracy'], label='train');
plt.plot(cb.get_evals_result()['validation']['Accuracy'], label='val');
plt.title('CatBoost Accuracy'); plt.legend(); plt.show()

# %% ───────────────────────────── TABNET CLASSIFIER ─────────────────────────────
print('\n▶️  Training TabNet …')
# encode categoricals
enc_X = X.copy(); cat_dims = {}
for c in cat_cols:
    le = LabelEncoder(); enc_X[c] = le.fit_transform(enc_X[c].astype(str)); cat_dims[c] = len(le.classes_)
cat_idxs = [X.columns.get_loc(c) for c in cat_cols]
cat_dims_list = [cat_dims[c] for c in cat_cols]
X_tab = enc_X.values.astype(np.float32)
X_tr, X_va, y_tr, y_va = train_test_split(X_tab, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED)

# class weights
cw = np.bincount(y_tr); cw = cw.max()/cw

net = TabNetClassifier(
    cat_idxs=cat_idxs, cat_dims=cat_dims_list, cat_emb_dim=8,
    n_d=16, n_a=16, n_steps=5, gamma=1.5, lambda_sparse=1e-4,
    optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2),
    scheduler_params={'gamma': 0.95, 'step_size': 20},
    scheduler_fn=torch.optim.lr_scheduler.StepLR,
    seed=RANDOM_SEED, verbose=20
)
net.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric=['accuracy'], max_epochs=200, patience=30, batch_size=1024, virtual_batch_size=128)

p_tab_prob = net.predict_proba(X_va); p_tab = p_tab_prob.argmax(axis=1)
print('\nTabNet  —  Acc:', accuracy_score(y_va, p_tab), 'Macro‑F1:', f1_score(y_va, p_tab, average='macro'))
print(classification_report(y_va, p_tab, digits=3))

# %% ───────────────────────────── SHAP EXPLAINABILITY ────────────────────────────
print('\n▶️  SHAP (CatBoost) …')
shap_vals = cb.get_feature_importance(Pool(X_val, label=y_val), type='ShapValues')
shap.summary_plot(shap_vals[:,:-1], X_val, show=False); plt.show()

# %% ───────────────────────────── OPTIONAL FAIRNESS ─────────────────────────────
for col in SENSITIVE_COLS:
    if col not in df.columns: continue
    print(f'\nSlice metrics for {col}:')
    for grp, idxs in df.groupby(col).groups.items():
        acc_grp = accuracy_score(y[idxs], cb.predict(X.iloc[idxs]))
        print(f'  {grp:<10} | Accuracy = {acc_grp:.3f}')
