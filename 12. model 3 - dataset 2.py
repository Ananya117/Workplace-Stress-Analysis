# -*- coding: utf-8 -*-
"""CEAT - model3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tJ0RCeKIyY4xs9sw0pMR54ldFirpuDYS
"""

# ⬇️ Run this once at the top of your Colab notebook
!pip install -qU optuna optuna-integration[catboost]

# 0️⃣  Library install

!pip install --quiet catboost optuna shap pytorch-tabnet openpyxl

# 1️⃣  Imports & basic config

import io, warnings, math
warnings.filterwarnings('ignore')
import numpy as np, pandas as pd, matplotlib.pyplot as plt, shap
from google.colab import files
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, f1_score, cohen_kappa_score
from catboost import CatBoostRegressor, CatBoostClassifier
import optuna
from optuna.integration import CatBoostPruningCallback
from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier
from sklearn.ensemble import StackingRegressor, StackingClassifier

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# 2️⃣  Upload the Excel file from your PC

print("⬆️  Upload your .xlsx file (≈10 k rows)…")
uploaded = files.upload()
assert uploaded, "No file uploaded!"
file_name = next(iter(uploaded))
print(f"✔️  Loaded {file_name}")

df = pd.read_excel(io.BytesIO(uploaded[file_name]))
print("Data preview:")
print(df.head())
print("Shape:", df.shape)

# 3️⃣  Basic sanity‑checks & setup

TARGET = 'stress_level'
if TARGET not in df.columns:
    raise ValueError(f"'{TARGET}' column not found in sheet!")

FEATURES = [c for c in df.columns if c != TARGET]
X, y = df[FEATURES], df[TARGET]


#📊  Correlation of features with the target
target_col = "stress_level"  # or "stress_level_encoded" if you're using that

# Drop non-numeric columns for correlation
numeric_df = df.select_dtypes(include='number')

# Calculate correlation
corr_table = numeric_df.corr()[[target_col]].drop(index=target_col)
corr_table = corr_table.sort_values(by=target_col, ascending=False)

# Display neatly
print("🔍  Correlation of features with target:")
display(corr_table.round(5))


MODE = 'regression'  # 🔄  change to 'classification' if 10 discrete labels
print("Running in", MODE.upper(), "mode…")

# Stratification buckets (needed even for regression)
strat_y = pd.cut(y, bins=[0,3,6,10], labels=[0,1,2]) if MODE=='regression' else y
X_tr, X_temp, y_tr, y_temp = train_test_split(X, y, test_size=0.30, stratify=strat_y, random_state=RANDOM_STATE)
X_val, X_te,  y_val, y_te  = train_test_split(X_temp, y_temp, test_size=0.50, stratify=strat_y.loc[y_temp.index], random_state=RANDOM_STATE)
print("Split →", X_tr.shape, X_val.shape, X_te.shape)

# 4️⃣  Hyper‑parameter tuning with Optuna
# -------------------------------------------------
def objective(trial):
    # Common hyper‑parameters for both tasks
    params = {
        "depth": trial.suggest_int("depth", 4, 8),
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 1, 30),
        "iterations": 5000,
        "random_seed": RANDOM_STATE,
        "verbose": 0,
    }

    # Choose CatBoost flavour
    if MODE == "regression":
        model = CatBoostRegressor(loss_function="MAE", **params)
    else:
        model = CatBoostClassifier(loss_function="MultiClass", **params)

    # Train with early stopping + Optuna pruning
    model.fit(
        X_tr,
        y_tr,
        eval_set=(X_val, y_val),
        early_stopping_rounds=300,
        use_best_model=True,
        verbose=False,
        callbacks=[
            CatBoostPruningCallback(
                trial, "MAE" if MODE == "regression" else "TotalF1"
            )
        ],
    )

    # Validation metric to minimise
    preds = model.predict(X_val)
    if MODE == "regression":
        return mean_absolute_error(y_val, preds)
    else:
        return 1 - f1_score(y_val, preds, average="macro")  # minimise 1‑F1


# Run the Optuna search
print("🔍  Optuna search (40 trials)…")
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=40, show_progress_bar=True)
print("Best params →", study.best_params)

# -------------------------------------------------
# Train the final CatBoost model on train+val
# -------------------------------------------------
best_params = dict(study.best_params)
best_params.update(
    {
        "iterations": 5000,
        "random_seed": RANDOM_STATE,
        "verbose": 200,
    }
)

if MODE == "regression":
    cb_model = CatBoostRegressor(loss_function="MAE", **best_params)
else:
    cb_model = CatBoostClassifier(loss_function="MultiClass", **best_params)

cb_model.fit(
    pd.concat([X_tr, X_val]),
    pd.concat([y_tr, y_val]),
    eval_set=(X_te, y_te),
    early_stopping_rounds=300,
    use_best_model=True,
)

#5️⃣  Train final CatBoost on train+val

cb_params = dict(study.best_params)
cb_params.update({'iterations':5000, 'random_seed':RANDOM_STATE, 'verbose':200})
cb_model = (CatBoostRegressor(loss_function='MAE', **cb_params) if MODE=='regression'
            else CatBoostClassifier(loss_function='MultiClass', **cb_params))
cb_model.fit(pd.concat([X_tr,X_val]), pd.concat([y_tr,y_val]),
             eval_set=(X_te, y_te), early_stopping_rounds=300, use_best_model=True)

#🔎 7A. OVER‑FITTING CHECK: LEARNING CURVE

import matplotlib.pyplot as plt
from math import isfinite

# Works for CatBoost models trained with eval_set:
if hasattr(cb_model, "evals_result_"):
    res = cb_model.evals_result_
    metric_key = "MAE" if MODE == "regression" else "MultiClass"
    train_curve = res["learn"][metric_key]
    val_curve   = res["validation"][metric_key]

    # Plot learning curves
    plt.figure(figsize=(6, 4))
    plt.plot(train_curve, label="Train")
    plt.plot(val_curve,  label="Validation")
    plt.title(f"Learning curve – {metric_key}")
    plt.xlabel("Trees")
    plt.ylabel(metric_key)
    plt.legend()
    plt.show()

    # Simple numeric gap test (rule‑of‑thumb: >5 % gap ⇒ possible over‑fit)
    gap = abs(train_curve[-1] - val_curve[-1])
    if isfinite(gap):
        pct_gap = gap / val_curve[-1]
        print(f"Δ final {metric_key} (train vs val): {gap:.4f} "
              f"({pct_gap*100:.1f} %)")
        if pct_gap > 0.05:
            print("⚠️  Potential over‑fitting (gap > 5 %). "
                  "Consider stronger regularisation or earlier stopping.")
        else:
            print("✅  No significant over‑fitting detected.")
else:
    print("No evals_result_ found ‑‑ did you pass an eval_set to CatBoost?")

# 6️⃣  TabNet + CatBoost Stacking  ────────────────────────────────────────
from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin
from sklearn.ensemble import StackingRegressor, StackingClassifier
from pytorch_tabnet.tab_model import TabNetRegressor, TabNetClassifier

# ── 6‑A. scikit‑learn‑compatible wrappers ───────────────────────────────
class TabNetRegWrapper(RegressorMixin, BaseEstimator):
    _estimator_type = "regressor"          # make sklearn happy

    def __init__(self, n_d=16, n_a=16, n_steps=5, seed=RANDOM_STATE):
        self.n_d, self.n_a, self.n_steps, self.seed = n_d, n_a, n_steps, seed

    def fit(self, X, y):
        self.model_ = TabNetRegressor(
            n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps, seed=self.seed
        )
        self.model_.fit(
            X_train=X,
            y_train=y.reshape(-1, 1),        # TabNet needs 2‑D target for regression
            max_epochs=200,
            patience=20,
            batch_size=1024,
            virtual_batch_size=128,
            num_workers=0,
            drop_last=False,
        )
        return self

    def predict(self, X):
        return self.model_.predict(X).ravel()


class TabNetClsWrapper(ClassifierMixin, BaseEstimator):
    _estimator_type = "classifier"

    def __init__(self, n_d=16, n_a=16, n_steps=5, seed=RANDOM_STATE):
        self.n_d, self.n_a, self.n_steps, self.seed = n_d, n_a, n_steps, seed

    def fit(self, X, y):
        self.model_ = TabNetClassifier(
            n_d=self.n_d, n_a=self.n_a, n_steps=self.n_steps, seed=self.seed
        )
        self.model_.fit(
            X_train=X,
            y_train=y,
            max_epochs=200,
            patience=20,
            batch_size=1024,
            virtual_batch_size=128,
            num_workers=0,
            drop_last=False,
        )
        return self

    def predict(self, X):
        return self.model_.predict(X)

    def predict_proba(self, X):
        return self.model_.predict_proba(X)


# ── 6‑B. stand‑alone TabNet training (optional) ──────────────────────────
TRAIN_TABNET = True
if TRAIN_TABNET:
    if MODE == "regression":
        tab_model = TabNetRegressor(n_d=16, n_a=16, n_steps=5, seed=RANDOM_STATE)
        tab_model.fit(
            X_train=X_tr.values,
            y_train=y_tr.values.reshape(-1, 1),
            eval_set=[(X_val.values, y_val.values.reshape(-1, 1))],
            max_epochs=200,
            patience=20,
            batch_size=1024,
            virtual_batch_size=128,
            num_workers=0,
            drop_last=False,
        )
    else:
        tab_model = TabNetClassifier(n_d=16, n_a=16, n_steps=5, seed=RANDOM_STATE)
        tab_model.fit(
            X_train=X_tr.values,
            y_train=y_tr.values,
            eval_set=[(X_val.values, y_val.values)],
            max_epochs=200,
            patience=20,
            batch_size=1024,
            virtual_batch_size=128,
            num_workers=0,
            drop_last=False,
        )

# ── 6‑C. stacking ensemble ──────────────────────────────────────────────
USE_STACK = TRAIN_TABNET
if USE_STACK:
    if MODE == "regression":
        stacker = StackingRegressor(
            estimators=[
                ("cat", cb_model),
                ("tab", TabNetRegWrapper(n_d=16, n_a=16, n_steps=5, seed=RANDOM_STATE)),
            ],
            final_estimator=CatBoostRegressor(
                loss_function="MAE", random_seed=RANDOM_STATE, verbose=0
            ),
        )
    else:
        stacker = StackingClassifier(
            estimators=[
                ("cat", cb_model),
                ("tab", TabNetClsWrapper(n_d=16, n_a=16, n_steps=5, seed=RANDOM_STATE)),
            ],
            final_estimator=CatBoostClassifier(
                loss_function="MultiClass", random_seed=RANDOM_STATE, verbose=0
            ),
        )

       # Fit the stacker on combined train + validation data  👇 NEW
    X_stack = np.vstack([X_tr.values, X_val.values])      # convert to NumPy
    y_stack = np.concatenate([y_tr.values, y_val.values]) # convert target to NumPy
    stacker.fit(X_stack, y_stack)                          # train ensemble

# 7️⃣  Evaluation on Test set ─────────────────────────────────────────────
import numpy as np
import math
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, accuracy_score, f1_score
)

models = {"CatBoost": cb_model}
if TRAIN_TABNET:
    models["TabNet"] = tab_model
if USE_STACK:
    models["Stack"] = stacker

print("\n================= Test metrics ================")
for name, mdl in models.items():
    # TabNet and the Stacker need NumPy, CatBoost works with either
    X_eval = X_te.values if name != "CatBoost" else X_te

    if MODE == "regression":
        pred = mdl.predict(X_eval)
        mae  = mean_absolute_error(y_te, pred)
        rmse = math.sqrt(mean_squared_error(y_te, pred))
        print(f"{name:8s} | MAE {mae:.3f} | RMSE {rmse:.3f}")
    else:
        pred = mdl.predict(X_eval)
        acc  = accuracy_score(y_te, pred)
        macro_f1 = f1_score(y_te, pred, average="macro")
        print(f"{name:8s} | Acc {acc:.3f} | Macro‑F1 {macro_f1:.3f}")

# Already have pred (continuous 1–10) from CatBoost / TabNet / Stack
# Bucket them into 3 stress levels or 10 discrete labels.
# Example: Low (1–3), Med (4–6), High (7–10)

def bucket(x):
    if x <= 3:   return 0          # Low
    elif x <= 6: return 1          # Medium
    else:        return 2          # High

y_true_cls = y_te.apply(bucket)
y_pred_cls = pd.Series(pred).apply(bucket)

from sklearn.metrics import accuracy_score, f1_score
print("Accuracy :", accuracy_score(y_true_cls, y_pred_cls))
print("Macro‑F1 :", f1_score(y_true_cls, y_pred_cls, average='macro'))

# 8️⃣  SHAP summary for CatBoost

print("\nSHAP…")
explainer = shap.TreeExplainer(cb_model)
shap_values = explainer.shap_values(X_te)
shap.summary_plot(shap_values, X_te)

print("✅  Done – scroll for plots & metrics.")
