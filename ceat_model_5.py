# -*- coding: utf-8 -*-
"""CEAT - model 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BwwpF2vbBy_jbo_g0rAQqpDzRl6OpXWp
"""

# 1. Install & import

!pip install catboost pytorch-tabnet imbalanced-learn shap --quiet

import pandas as pd
import numpy as np
from google.colab import files
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.impute import KNNImputer, SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.metrics import (accuracy_score, f1_score, precision_score,
                             recall_score, classification_report, confusion_matrix)
from catboost import CatBoostClassifier, Pool
from pytorch_tabnet.tab_model import TabNetClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
import shap
import matplotlib.pyplot as plt
import seaborn as sns

RANDOM_STATE = 42

# 2. Upload CSV

print("\nüëâ Select your CSV file‚Ä¶")
uploaded = files.upload()

# Assumes only one file is uploaded
csv_path = next(iter(uploaded))
print(f"Loaded: {csv_path}")

df = pd.read_csv(csv_path)

# 3. Basic EDA snapshot (optional ‚Äì quick sanity check)

print("\nüîé Dataset head:")
print(df.head())
print("\nüìù Shape:", df.shape)
print("\nüîç Missing values per column:\n", df.isna().sum())
print("\nüìä Target distribution:")
print(df['Stress_Level'].value_counts().sort_index())

# OPTIONAL: Outlier capping & feature engineering
# IQR‚Äëbased capping for numeric columns
for col in ['Sleep_Duration', 'Emails_Sent']:
    Q1, Q3 = df[col].quantile([0.25, 0.75])
    iqr = Q3 - Q1
    lower, upper = Q1 - 1.5 * iqr, Q3 + 1.5 * iqr
    df[col] = df[col].clip(lower, upper)

# Feature engineering
df['Work_Intensity'] = df['Emails_Sent'] / (df['Daily_Working_Hours'] + 1e-3)
df['Screen_to_Sleep_Ratio'] = df['Screen_Time'] / (df['Sleep_Duration'] + 1e-3)

# 4. Pre‚Äëprocessing pipeline

# Drop identifier
if 'Employee_ID' in df.columns:
    df = df.drop('Employee_ID', axis=1)

# Define features/target
TARGET = 'Stress_Level'
X = df.drop(TARGET, axis=1)
y = df[TARGET]

# Identify column types (numerical vs categorical)
# Treat Work_Life_Balance_Satisfaction as categorical if integer 1‚Äë5
categorical_cols = ['Work_Life_Balance_Satisfaction'] if 'Work_Life_Balance_Satisfaction' in X.columns else []
numerical_cols = [c for c in X.columns if c not in categorical_cols]

numeric_transformer = Pipeline(steps=[
    ("imputer", KNNImputer(n_neighbors=5)),
    ("scaler", MinMaxScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numerical_cols),
        ("cat", categorical_transformer, categorical_cols)
    ])

# 5. Train‚Äëtest split + SMOTE (only on train data)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)

# Fit‚Äëtransform training data, transform test data
X_train_enc = preprocessor.fit_transform(X_train)
X_test_enc  = preprocessor.transform(X_test)

# Address class imbalance (if any)
print("\n‚öñÔ∏è  Applying SMOTE to balance classes‚Ä¶")
smote = SMOTE(random_state=RANDOM_STATE)
X_train_res, y_train_res = smote.fit_resample(X_train_enc, y_train)
print("After SMOTE, class counts:")
print(pd.Series(y_train_res).value_counts())

# 6. Model training
import torch

# 6.1 CatBoost (fast + explainable)
cat_model = CatBoostClassifier(
    iterations=1000,
    depth=6,
    learning_rate=0.05,
    loss_function='MultiClass',
    eval_metric='Accuracy',
    random_seed=RANDOM_STATE,
    verbose=False,
    early_stopping_rounds=50
)
print("\nüöÄ Training CatBoost‚Ä¶")
cat_model.fit(
    X_train_res, y_train_res,
    eval_set=(X_test_enc, y_test),
    use_best_model=True
)

# 6.2 TabNet (attention‚Äëbased DL for tabular)
print("\nüöÄ Training TabNet‚Ä¶ (will take ~1‚Äë2¬†min on CPU)")
# Convert to numpy
X_train_tab = X_train_res.astype(np.float32)
X_test_tab  = X_test_enc.astype(np.float32)

tabnet_model = TabNetClassifier(
    n_d=16, n_a=16,
    n_steps=5,
    gamma=1.5,
    n_independent=2, n_shared=2,
    optimizer_fn=torch.optim.Adam,
    optimizer_params=dict(lr=2e-2),
    scheduler_params={"milestones": [200, 400], "gamma": 0.5},
    scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,
    mask_type='entmax',
    verbose=0,
    seed=RANDOM_STATE
)

tabnet_model.fit(
    X_train_tab, y_train_res.values,
    eval_set=[(X_test_tab, y_test.values)],
    eval_name=["val"],
    eval_metric=["accuracy"],
    patience=25,
    max_epochs=500
)

# 6.3 Simple Stacking Ensemble
stack = StackingClassifier(
    estimators=[('cat', cat_model), ('tab', tabnet_model)],
    final_estimator=LogisticRegression(max_iter=1000),
    passthrough=False,
    n_jobs=-1
)
print("\nüöÄ Training Stacking Ensemble‚Ä¶")
stack.fit(X_train_res, y_train_res)

# 7. Evaluation helper function

def evaluate(model, X_tr, y_tr, X_te, y_te, label="Model"):
    y_pred_train = model.predict(X_tr)
    y_pred_test  = model.predict(X_te)

    print(f"\n===== {label} Results =====")
    for split, y_true, y_pred in [("TRAIN", y_tr, y_pred_train), ("TEST", y_te, y_pred_test)]:
        acc = accuracy_score(y_true, y_pred)
        f1  = f1_score(y_true, y_pred, average='weighted')
        prec = precision_score(y_true, y_pred, average='weighted')
        rec  = recall_score(y_true, y_pred, average='weighted')
        print(f"{split} ‚Äî Acc: {acc:.4f} | F1: {f1:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f}")
    print("Classification Report (TEST):\n", classification_report(y_te, y_pred_test))
    cm = confusion_matrix(y_te, y_pred_test)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix ‚Äî {label}')
    plt.xlabel('Predicted'); plt.ylabel('True');
    plt.show()

# Evaluate all three
evaluate(cat_model, X_train_res, y_train_res, X_test_enc, y_test, label="CatBoost")

evaluate(tabnet_model, X_train_tab, y_train_res, X_test_tab, y_test, label="TabNet")

evaluate(stack, X_train_res, y_train_res, X_test_enc, y_test, label="Stacking Ensemble")

# =============================================================
# üîÑ Extra Models + Unified Metric Table  (0‚Äëbased labels fix)
# =============================================================

# 0) Install extra packages (first run only)
!pip install xgboost lightgbm --quiet

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier

# -----------------------------------------------------------------
# A. Prep data
# -----------------------------------------------------------------
# 1) Dense arrays for models that dislike sparse input
X_tr_dense = X_train_res.toarray() if hasattr(X_train_res, "toarray") else X_train_res
X_te_dense = X_test_enc.toarray()  if hasattr(X_test_enc,  "toarray") else X_test_enc

# 2) Shift labels to 0‚Äëbased (XGBoost/LightGBM requirement)
y_train_res_adj = y_train_res - y_train_res.min()   # e.g. 1‚Äë10 ‚Üí 0‚Äë9
y_test_adj      = y_test      - y_test.min()

# 3) Storage for summary metrics
summary = {}

def eval_and_store(model, label):
    """Run evaluate() and stash TEST metrics for summary table."""
    y_pred = model.predict(X_te_dense)
    acc  = accuracy_score(y_test_adj, y_pred)
    f1   = f1_score(y_test_adj, y_pred, average='weighted')
    prec = precision_score(y_test_adj, y_pred, average='weighted')
    rec  = recall_score(y_test_adj, y_pred, average='weighted')
    summary[label] = dict(Accuracy=acc, F1=f1, Precision=prec, Recall=rec)
    evaluate(model, X_tr_dense, y_train_res_adj, X_te_dense, y_test_adj, label=label)

# -----------------------------------------------------------------
# B. Train individual models
# -----------------------------------------------------------------
# 1) XGBoost
xgb_model = XGBClassifier(
    n_estimators=600, max_depth=6, learning_rate=0.05,
    subsample=0.8, colsample_bytree=0.8,
    objective="multi:softprob",
    num_class=len(np.unique(y_train_res_adj)),
    random_state=RANDOM_STATE, n_jobs=-1
)
xgb_model.fit(X_tr_dense, y_train_res_adj)
eval_and_store(xgb_model, "XGBoost")

# 2) LightGBM
lgb_model = LGBMClassifier(
    n_estimators=800, learning_rate=0.05,
    num_leaves=64, objective="multiclass",
    random_state=RANDOM_STATE, n_jobs=-1
)
lgb_model.fit(X_tr_dense, y_train_res_adj)
eval_and_store(lgb_model, "LightGBM")

# 3) ExtraTrees
et_model = ExtraTreesClassifier(
    n_estimators=500, max_depth=None,
    random_state=RANDOM_STATE, n_jobs=-1
)
et_model.fit(X_tr_dense, y_train_res_adj)
eval_and_store(et_model, "ExtraTrees")

# 4) MLP (feed‚Äëforward NN)
mlp_model = MLPClassifier(
    hidden_layer_sizes=(128, 64),
    activation="relu", solver="adam",
    learning_rate_init=1e-3, max_iter=300,
    random_state=RANDOM_STATE
)
mlp_model.fit(X_tr_dense, y_train_res_adj)
eval_and_store(mlp_model, "MLP")

# 5) Soft‚ÄëVoting Ensemble (tree models)
vote_model = VotingClassifier(
    estimators=[("xgb", xgb_model), ("lgb", lgb_model), ("et", et_model)],
    voting="soft", n_jobs=-1
)
vote_model.fit(X_tr_dense, y_train_res_adj)
eval_and_store(vote_model, "VotingEnsemble")

# -----------------------------------------------------------------
# C. Show metric table
# -----------------------------------------------------------------
import pandas as pd
metric_df = (pd.DataFrame(summary)
             .T.round(4)
             .sort_values("Accuracy", ascending=False))
print("\nüîÆ  Test‚Äëset Metric Summary:")
display(metric_df)

# 8. Quick over‚Äëfitting check via 5‚Äëfold CV on Stacking model

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
cv_scores = cross_val_score(stack, X.values, y.values, cv=cv, scoring='accuracy', n_jobs=-1)
print("\nüîÅ 5‚Äëfold CV Accuracy (Stack):", cv_scores)
print("Mean ¬± SD:", cv_scores.mean().round(4), "¬±", cv_scores.std().round(4))

# 9. SHAP Explainability on CatBoost

print("\nüîç Calculating SHAP values (CatBoost)‚Ä¶")
shap.initjs()
explainer = shap.TreeExplainer(cat_model)
# To speed up, use a sample of the training data
sample_idx = np.random.choice(range(X_train_res.shape[0]), size=1000, replace=False)
shap_values = explainer.shap_values(X_train_res[sample_idx])

# Summary plot (feature importance)
shap.summary_plot(shap_values, X_train_res[sample_idx], feature_names=preprocessor.get_feature_names_out(), show=False)
plt.tight_layout()
plt.show()